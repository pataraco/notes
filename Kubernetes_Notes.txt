----------------------
Kubernetes (k8s) Notes
----------------------

My kubernetes notes.

Questions
----------
1) Are you ready to handle k8s release schedule?
  - major releases: 4 x year
  - minor releases: 12+ x year
  - support: N-2 ~ 9 months
2) How many clusters will you need?
  - need to be ready to expand/deploy to (N)
  - 1 cluster / team|project
3) How mature is your SDN (SW Defined Network)?
  - L7/LB

TODO
----
- https://github.com/ahmetb/kubectx
- https://kubernetes.io/docs/reference/kubectl/cheatsheet/
- https://kubernetes.io/docs/reference/kubectl/jsonpath/
- source <(kubectl completion bash)

Components
----------

Master Components
-----------------
control plane. global decisions. events.

 - kube-apiserver: exposes API, scales
 - kube-controller-manager: runs controllers
   - node controller: notices/responds to nodes going down
   - replication controller: maintains correct number of pods for replication
     controller objects
   - endpoints controller: populates the Endpoints object (joins Services & Pods)
   - service account & token controllers: create default accounts and API access
     tokens for new namespaces
 - kube-scheduler: schedules newly created pods to run on specific nodes
 - etcd: key/val store of cluster data, always backup
 - controller: control loop that watches the shared state of the cluster
     through the apiserver and makes changes attempting to move the current
     state towards the desired state.
 - cloud-controller-manager: runs controllers that interact with the underlying
     cloud providers
   - node controller: checks cloud provider it a node has been deleted in the
     cloud after it stops responding
   - route controller: set up routes in the underlying cloud infra
   - service controller: create, update, deleted cloud provider load balancers
   - volume controller: create, attach, mount volumes and interacting with the
     cloud provider to orchestrate volumes

Node Components
---------------
run on every node, maintain running pods, provide k8s runtime environment

 - kubelet: agent, makes sure that containers are running in a pod with PodSpecs
 - kube-proxy: enables the k8s service abstraction by maintaining network rules
   on the host and performing connection forwarding
 - container runtime: software responsible for running containers (docker,
   containerd, cri-o, rktlet, and any k8s CRI (container Runtime interface))


Addons
------
pods and services that implement cluster features. managed by deployments,
replication controllers, etc.

 - extended list:
   https://kubernetes.io/docs/concepts/cluster-administration/addons/

 - DNS: all k8s clusters should have cluster DNS. DNS server, serves DNS records
   for k8s services. containers started by k8s automatically include this DNS
   server in the DNS searches
   <service-name>.<namespace-name>.svc.cluster.local
 - Web UI (dashboard): general purpose, web-based UI for k8s clusters. allows
   management, troubleshooting of cluster applications and the cluster
   (https://github.com/kubernetes/dashboard)

   1. To deploy Dashboard, execute following command:
      $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
   2. To access Dashboard from locahost create a secure channel to the Kubernetes cluster. Run the following command:
      $ kubectl proxy
   3. Now access Dashboard at:
      http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/.

 - Container resource monitoring: records generic time-series container metrics
   in a central db and a UI to browse the data
 - Cluster-level Logging: saves container logs to a central log store with
   search/browsing interface

Objects
-------
persistent entities in the k8s system which represent the state of the cluster
 - which containerized apps are running and on which nodes
 - resources available to those applications
 - applications policies: restart, upgrades, fault-tolerance
 - "record of intent": k8s contantly work to ensure that create objects exist
   k8s desired state
 - use k8s API to create, modify, delete objects
 - object spec: describes desired state - characteristics
 - object status: actual state
 - required fields:
   - apiVersion: version of the k8s API using to create the object
   - kind: kind of object to create
   - metadata: data to uniquely identify the object, including a `name`, `UID`
     and `namespace`
   - spec: precise object format
   - name: client provided
   - UID: k8s system generated
   - namespace: virtual cluster backed by the same physical cluster, unique
     objects within
   - labels/selectors: identifying attributes, organize, select
   - annotations: arbitrary, non-identifying metadata. uri's, version, etc.
   - field selectors: select resources based on the value of one or more
     resource fields (metadata.name=my_service, metadata.namespace!=default)


   Spec formats:
   - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/

Architecture
------------
 - Nodes: worker machine (minion). managed by the master components.
   - status:
     - addresses: HostName, ExternlIp, InternalIp
     - condition: OutOfDisk, Ready, MemoryPressure, PIDPressure, DiskPressure, NetworkUnavailable
     - capacity: resouces available on the node: CPU, memory, max number of pods
     - info: general info, kernel version, k8s version, Docker version, O.S. name
   - not inherently created by k8s. created externally by cloud provider
   - interacting components: node controller, kubelet, kubectl
   - registration: with kubelet flag --register-node (default)
 - Master-Node communication

Containers
----------
   - Images
   - Container Environment Variables
   - Runtime Class
   - container Lifecycle Hooks

Workloads
---------
 - Pods: managed by controllers
 - Controllers:
   - ReplicaSet: maintains a stable set of replica Pods running, owner of a set
     of Pods
   - ReplicationController: ensures specified number of pod replicas are running
   - Deployments: roll out ReplicaSets, declare pods states, rollback, scale,
     pause, statuses, cleanup
     (a Deployment that configures a ReplicaSet in now the recommended way to
      set up replication)
   - StatefulSets: stable, unique network identifiers, persistent storage,
     ordered, graceful deployments and scaling, automated rolling updates
   - DaemonSet: ensures that all (or some) Nodes run a copy of a Pod, run cluster
     storage daemon, run logs collection daemon on every node, run monitoring
     daemon on every node. one set per each type of daemon
   - Garbage Collection: deletes certain objects (dependents) that once had an
     owner, but no longer have an owner. metadata.ownerReferences
   - TTL Controller: limit the lifetime of resource objects that have finished
     execution (currently only handles jobs)
   - Jobs: creates one or more Pods and ensures that a specified number of them
     successfully terminate
   - CronJob: create Jobs on a time-based schedule

Services, Load Balancing, Networking
------------------------------------
https://kubernetes.io/docs/concepts/services-networking/service
https://kubernetes.io/docs/tutorials/services/source-ip/
https://kubernetes.io/docs/concepts/services-networking/connect-applications-service

 - Services: defines a logical set of Pods and access policy determined by
   Label Selector
 - DNS for Services and Pods
 - Connnectiong Applications with Services
 - Ingress
 - Ingress controllers
 - Network Policies
 - Adding entries to Pod /etc/hosts with HostAliases
 - ClusterIP -> NodePort -> LoadBalancer, ExternalName

Building Blocks (Primary)
-------------------------
if you can explain what each one of these do and how they build on each other,
then you'll have a good grasp of k8s:

 - Pod
 - Service
 - Volume
 - Namespace
 - Container
 - ReplicaSet or Controller
 - Deployment
 - Ingress Rule

Building Blocks (Secondary)
---------------------------
 - Namespaces
 - Configmap
 - Secrets
 - Limits
 - env and envFrom
 - Node and Pod Affinity and Antiaffinity
 - Resource limits
 - Daemon Set
 - Stateful Set
 - Persistent Volume
 - Persistent Volume Claim


Installation
------------
brew install kubernetes-cli  (github.com/kubernetes/kubectl.git)
   (or install with [Docker Desktop](www.docker.com/products/docker-desktop))
kubectl version [--short]  # get client and server versions
brew install kops (github.com/kubernetes/kops.git)
brew cask install virtualbox
brew cask install minikube
minikube version
minikube start
minikube start \
    --network-plugin=cni \
    --enable-default-cni \
    --container-runtime=containerd \
    --container-runtime=cri-o \
    --container-runtime=rkt \
    --bootstrapper=kubeadm
kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080
kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.10 --port=8080
kubectl run --generator=run-pod/v1 echoserver --image=k8s.gcr.io/echoserver --port=8080  # only creates a pod
     docker run -p 8085:8080 k8s.gcr.io/echoserver:1.10
     docker run -p 8086:8080 gcr.io/google_containers/echoserver:1.4
kubectl expose deployment hello-minikube --type=NodePort
kubectl get pods
minikube service hello-minikube --url
kubectl delete deployment hello-minikube
kubectl delete service hello-minikube
minikube addons list
minikube addons enable ADDON
minikube stop

Kubeadm
-------
kubeadm init
kubeadm token create --print-join-command

Configuration
-------------
EKS
---
1. create EKS cluster (creates master nodes)
2. create worker nodes
3. enable worker nodes to join the cluster (via ConfigMap)
   e.g. docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html
   --- aws-auth-cm.yaml (begin) ---
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: aws-auth
     namespace: kube-system
   data:
     mapRoles: |
       - rolearn: <ARN of instance role>
         username: system:node:{{EC2PrivateDNSName}}
         groups:
           - system:bootstrappers
           - system:nodes
   --- aws-auth-cm.yaml (end) ---
   kubectl apply -f aws-auth-cm.yaml


Role-Base Access Control (RBAC)
-------------------------------
[Using RBAC Authorizaton](https://kubernetes.io/docs/reference/access-authn-authz/rbac)
[Configure RBAC In Your Kubernetes Cluster](https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster)

- Resource Types: Role, ClusterRole, RoleBinding, ClusterRoleBinding


Service Accounts
----------------
[Service Accounts](https://kubernetes-on-aws.readthedocs.io/en/latest/user-guide/service-accounts.html)
- pods/apps use service accounts to authenticate when interacting with the API server
- by default, apps use the default service account in their namespace
- apps are authed to perform certain actions based on the service account selected
- currently allowed:
  - kube-system:system   # used only for amin access in kube-system namespace
  - kube-system:default  # used only for read only access in kube-system namespace
  - default:default      # gives read only access to the API
  - *:operator           # gives full access to the used namespace and read-write access to TPR, storage classes, and persistent volumes in all namespaces


Commands
--------
Kubectl book: https://kubectl.docs.kubernetes.io/

Reference:
  https://kubernetes.io/docs/reference/

 - Imperative vs. Declaritive

kubectl get      - list one or many resources.
kubectl describe - show detailed information about a specific or many resources.
kubectl logs     - print the logs from a container in a pod or specified resource.
kubectl exec     - execute a command on a container in a pod
kubectl apply    - execute a command on a container in a pod

 - minikube ip
 # API of the pod
 - curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/

 - kubectl $CMD [--help] [-o wide|json|yaml] [-l $KEY=$VAL]
 - kubectl api-resources [--namespaced=true|false]  #  list supported API resources
 - kubectl attach $POD/$TYPE/NAME [-c $CONTAINER]
 - kubectl auth can-i|reconcile
 - kubectl auth reconcile -f $MANIFEST [--dry-run]
 - kubectl cluster-info  # get CIDR:port of master and KubeDNS info
 - kubectl config current-context
 - kubectl config get-clusters
 - kubectl config get-contexts
 - kubectl config set-context $(kubectl config current-context) --namespace=$NAMESPACE
 - kubectl config set-context --current --namespace=$NAMESPACE
 - kubectl config use-context $CONTEXT
 - kubectl config use-context minikube
 - kubectl config view
 - kubectl config view --minify | grep namespace:
 - kubectl create secret generic $SECRET_NAME --from-literal=$KEY=$VAL [-n $NAMESPACE]
 - kubectl delete secret $SECRET_NAME [-n $NAMESPACE]
 - kubectl delete service $SERVICE
 - kubectl delete service -l $KEY=$VAL
 - kubectl describe daemonset aws-node -n kube-system [| grep Image]
 - kubectl describe deployment coredns -n kube-system [| grep Image]
 - kubectl describe configmap aws-auth -n kube-system
 - kubectl describe namespace [$NAMESPACE_NAME]
 - kubectl describe pods [$POD_NAME]
 - kubectl describe secrets [$SECRET]
 - kubectl describe services [$SERVICE] [-n $NAMESPACE]
 - kubectl describe services/$SERVICE
 - kubectl exec -it $POD_NAME curl localhost:8080
 - kubectl exec [-it] $POD_NAME $CMD
 - kubectl explain RESOURCE [options]
 - kubectl expose deployment/$DEPOLYMENT --type NodePort --port 8080
 - kubectl expose deployment/$DEPOLYMENT -n NS --name NAME --type NodePort --port 80 --target-port 80 
 - kubectl expose deployment/$DEPOLYMENT --type=NodePort --port=8080
 - kubectl expose deployment $DEPOLYMENT --port=8080 --target-port=80 --name nginx
 - kubectl get --raw <URL>
 - kubectl get --raw /api/v1/namespaces | jq .items[].metadata.name
 - kubectl get all -n NAMESPACE  # get all resources in a specific namespace
 - kubectl get configmap -o json
 - kubectl get deployments [-A|--all-namespaces]
 - kubectl get deploy DEPLOY [-A|--all-namespaces]
 - kubectl get events
 - kubectl get namespace
 - kubectl get nodes
 - kubectl get pods -n kube-system -l k8s-app=kube-dns
 - kubectl get pods --context=minikube
 - kubectl get pods --field-selector status.phase=Running
 - kubectl get pods --field-selector status.phase=Running,spec.restartPolicy=Always
 - kubectl get pods -l 'env in (prod),tier in (web)'
 - kubectl get pods -l 'env in (prod, qa),tier in (web)'
 - kubectl get pods -l 'env,env notin (qa)'
 - kubectl get pods -l env==prod,tier==web
 - kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}'
 - kubectl get pods [-n $NAMESPACE | --namespace=$NAMESPACE]
 - kubectl get secrets
 - kubectl get services
 - kubectl get services/$SERVICE -o go-template='{{(index .spec.ports 0).nodePort}}'
 - kubectl get statefulsets,services --field-selector metadata.namespace!=default
 - kubectl port-forward TYPE/NAME [options] [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N]
 - kubectl label <OBJECT> $OBJECT_NAME $KEY=$VAL
 - kubectl logs $POD_NAME
 - kubectl proxy
 - kubectl rollout history deploy/DEPLOY
 - kubectl rollout status deploy/DEPLOY
 - kubectl scale deployments $DEPLOYMENT --replicas=$X
 - kubectl taint nodes NODE_NAME KEY=VALUE:EFFECT
 - kubectl top node  # needs metrics-server
 - kubectl top pod   # needs metrics-server

JSON Path
-----------------
- https://kubernetes.io/docs/reference/kubectl/jsonpath/

$ kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.capacity.cpu}{"\n"}{end}'
$ kubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu
$ kubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu --sort-by .metadata.name
$ kubectl get pods POD_NAME -o jsonpath='{.status.containerStatuses[?(@.name=='redis-container')].restartCount}'
$ kubectl get nodes -o jsonpath='{.items[*].metadata.name}'  # node_names
$ kubectl get nodes -o jsonpath='{.items[*]status.nodeInfo.osImage}'  # nodes_os
$ kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}'  # nodes_os
$ kubectl config view --kubeconfig=./my-kube-config -o jsonpath='{.users[*].name}'  # users
$ kubectl get pv -o json --sort-by .spec.capacity.storage  # storage-capacity-sorted
$ kubectl get pv --sort-by .spec.capacity.storage -o custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage  # pv-and-capacity-sorted
$ kubectl config view --kubeconfig=./my-kube-config -o jsonpath='{.contexts[?(@.context.user=="aws-user")].name}'  # aws-context-name
$ kubectl -n admin2406 get deploy --sort-by .metadata.name -o custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:spec.template.spec.containers[0].image,READY_REPLICAS:status.readyReplicas,NAMESPACE:metadata.namespace

Manifest Creations
--------
Use the `--dry-run=client` and `-o yaml` options to produce, e.g.

$ kubectl run NAME --image=IMAGE --dry-run=client -o yaml      # Pod
$ kubectl create deploy NAME --image=IMAGE --dry-run=client -o yaml   # Deployment
$ kubectl create resourcequota NAME -n NS \                    # ResourceQuota
     --hard=pods=5,requests.cpu=3,requests.memory=2Gi --dry-run=client -o yaml

Static Pods
-----------
# static pods should be named with "-NODE_NAME"
$ kubectl get pods | grep -- '-NODE_NAME'

# find static directory
pgrep kublet | grep -- '--config'
grep -i 'static' STATIC_CONFIG_FILE

Node Taint, Toleration, Selection, Affinity
------------
$ kubectl taint nodes NODE_NAME KEY=VALUE:EFFECT
Effects: NoSchedule, PreferNoSchedule, NoExecute

# pod manifests:
tolerations: [{key:KEY, operator:Equal, value:VALUE, effect:NoSchedule}]
tolerations: [{key:KEY, operator:Exists, effect:NoSchedule}]

affinity:
  nodeAffinity:

nodeSelector:
   KEY: VALUE
   KEY2: VALUE2

nodeName: NODE_NAME # schedule pod to specific node

API
---
API Conventions:
  https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md
API Overview:
  https://kubernetes.io/docs/reference/using-api/api-overview/
Access:
  https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
Python Client:
  https://github.com/kubernetes-client/python
Groups (apiVersion):
  core/legacy: REST: /api/v1                     apiVersion: v1
  named:       REST: /apis/$GROUP_NAME/$VERSION  apiVersion: $GROUP_NAME/$VERSION

"selector": {
    "component" : "redis"
}

Snapshot etcd
------------------
ETCDCTL_API=3 etcdctl \   # verify correctness
   --endpoints=https://[127.0.0.1]:2379 \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key \
   member list

ETCDCTL_API=3 etcdctl \
   --endpoints=https://[127.0.0.1]:2379 \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key \
   snapshot save /path/to/FILE.db

ETCDCTL_API=3 etcdctl \
   --endpoints=https://[127.0.0.1]:2379 \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt \
   --name=master \                                         # your choice
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key \
   --data-dir /var/lib/etcd-from-backup \                  # your choice
   --initial-cluster=master=https://127.0.0.1:2380 \
   --initial-cluster-token=etcd-cluster-1 \                # your choice
   --initial-advertise-peer-urls=https://127.0.0.1:2380 \
   snapshot restore /path/to/FILE.db

Labels & Selectors
------------------

Recommended Labels
------------------
https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/

a common set of labels allows tools to work interoperably, describing object in
a common manner that all tools can underststand. and describe applications in a
way that can be queried

 - organized around the cnocept of an application
 - applications described with metadata
 - shared labels and annotations share a common prefix: `app.kubernetes.io`
 - labels without a prefix are private to users
 - a shared prefix ensures that shared labels do not interfere with custom user
   labels

selector:
    component: redis

selector:
  matchLabels:
    componect: redis
  matchExpressions:
    - {key: env, operator: In, values: [prod, qa]}
    - {key: tier, operator: NotIn, values: [web]}

(operators: In, NotIn, Exists, DoesNotExist)

Issues
------

## Upgrading Amazon EKS

Because Amazon EKS runs a highly available control plane, it can only be updated by one minor version at a time. See [Updating an Amazon EKS Cluster Kubernetes Version](https://docs.aws.amazon.com/en_pv/eks/latest/userguide/update-cluster.html) for additional information.

1. get/compare versions of server/client/nodes
  - server (control plane) and client: `kubectl version --short`
  - worker nodes: `kubectl get nodes`
  - if the worker nodes are more than one minor version older than the control plane, upgrade the worker nodes first to the same current version of the control plane (they cannot be on a newer version)
2. upgrade worker nodes (if applicable)
  if using Terraform AWS EKS module, when you specify a newer version and `terraform apply` and don't specify which AMI to use, a new AWS Launch Comfiguration/Template will get generated using the newer AMI and attach it to the ASG.
  it's recommended to use the 'ami_id' attribute/setting for the worker_groups variable to specify which AMI to use so that no unexpected updates occur.
  - get latest AWS AMI's (e.g. for version 1.12)
    - `aws ec2 describe-images --region=us-west-2  --filters Name=name,Values=*eks*node*1.12* --query "Images[].[Name,CreationDate,State]" --output`
  - use the new AMI to create new worker nodes and deploy either via terraform or manually and add to the cluster and/or AWS ASG
  - drain the old worker nodes and taint them to prevent any pods from being deployed onto them and then terminate them when all the pods have been moved onto new nodes
  - If using AWS ASG, terminate the old/drained instances (worker nodes)
3. move pods (if applicable)
  * a) with AWS AutoScaling (included with terraform-aws-modules/terraform-aws-eks)
    - increase the ASG desired count and wait for the new nodes to be ready
    - drain the old nodes (instances) to move the pods to the new nodes and taint them to prevent any new deployments onto themand wait for all the pods to be moved off of the old nodes
    - protect the new instances from scale-in in AWS ASG and unprotect the old
    - decrease the ASG desired count back to normal and let AWS ASG terminate the old instances
  * b) **Or** with `cluster-autoscaler`, it may be good to increase the AWS ASG desired count to launch new instances (with the newer version) and `cluster-autoscaler` may handle the draining/moving/terminatiion.
    - **TODO**: this needs to be tested
4. upgrade EKS cluster version
  * a) Terraform v0.11.x and AWS EKS module (terraform-aws-modules/terraform-aws-eks)
    - you can update/specify to the next higher version and terraform will update AWS EKS and the AWS ASG and Launch Configuration/Template (if an ami_id is not specified or the ami_id is specified and has been updated).
  * b) **Or** via AWS CLI
    - update: `aws eks --region $REGION update-cluster-version --name $CLUSTER_NAME --kubernetes-version $VERSION`
    - status: `aws eks --region $REGION describe-update --name $CLUSTER_NAME --update-id $UPDATE_ID`  # (update ID obtained from previous `update-cluster-version` command)
5. check/set the kube-proxy daemonset image to match kubernetes version _(see table below_)
   - check: `kubectl describe ds kube-proxy -n kube-system | grep Image`
   - patch: `kubectl set image daemonset.apps/kube-proxy -n kube-system kube-proxy=602401143452.dkr.ecr.us-west-2.amazonaws.com/eks/kube-proxy:v1.12.6`
6. check if your cluster is running CoreDNS
   - check: `kubectl get pod -n kube-system -l k8s-app=kube-dns`
7. check/set version of coredns deployment (_see table below_)
   - check: `kubectl describe deployment coredns -n kube-system | grep Image`
   - patch: `kubectl set image deployment.apps/coredns -n kube-system coredns=602401143452.dkr.ecr.us-west-2.amazonaws.com/eks/coredns:v1.2.2`
8. check/set version of cluster's Amazon VPC CNI Plugin for Kubernetes (should be > 1.5.3)
   - check: `kubectl describe daemonset aws-node --namespace kube-system | grep Image`
   - patch: `kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.5/config/v1.5/aws-k8s-cni.yaml`

### Versions Table

|Kubernetes|KubeProxy|CoreDNS|
|-|-|-|
|1.12|1.12.6|1.2.2|
|1.13|1.13.10|1.2.6|
|1.14|1.14.6|1.3.1|

tags: kubernetes, k8s
